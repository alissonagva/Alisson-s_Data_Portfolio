{"cells":[{"source":"## Exploratory Data Analysis in Python","metadata":{},"cell_type":"markdown","id":"fa1c7a29-da2f-4716-8cd5-c1ffa7f4f210"},{"source":"# Print the first five rows of unemployment\nprint(unemployment.head())\n\n# Count the values associated with each continent in unemployment\nprint(unemployment[\"continent\"].value_counts())\n\n# Import the required visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of 2021 unemployment; show a full percent in each bin\nsns.histplot(data=unemployment, x=\"2021\", binwidth=1)\n\nplt.show()","metadata":{},"cell_type":"code","id":"0a768d52-e963-402b-aa5e-64dba52d7bdf","outputs":[],"execution_count":null},{"source":"# Define a Series describing whether each continent is outside of Oceania\nnot_oceania = ~unemployment[\"continent\"].isin([\"Oceania\"])\n\n# Print the minimum and maximum unemployment rates during 2021\nprint(unemployment[\"2021\"].min(), unemployment[\"2021\"].max())\n\n# Create a boxplot of 2021 unemployment rates, broken down by continent\nsns.boxplot(data=unemployment, x=\"2021\", y=\"continent\")\nplt.show()","metadata":{},"cell_type":"code","id":"4b4fb226-7776-425f-95e9-6dd98b81b710","outputs":[],"execution_count":null},{"source":"# Print the mean and standard deviation of rates by year\nprint(unemployment.agg([\"mean\", \"std\"]))\n\n# Print yearly mean and standard deviation grouped by continent\nprint(unemployment.groupby(\"continent\").agg([\"mean\", \"std\"]))\n\ncontinent_summary = unemployment.groupby(\"continent\").agg(\n    # Create the mean_rate_2021 column\n    mean_rate_2021=(\"2021\", \"mean\"),\n    # Create the std_rate_2021 column\n    std_rate_2021=(\"2021\", \"std\")\n)\nprint(continent_summary)\n\n# Create a bar plot of continents and their 2021 average unemployment\nsns.barplot(data=unemployment, x=\"continent\", y=\"2021\")\nplt.show()","metadata":{},"cell_type":"code","id":"1f7a2a19-3a0a-45c7-8796-b6572b2845e5","outputs":[],"execution_count":null},{"source":"## Chapter 2","metadata":{},"cell_type":"markdown","id":"3a550f67-0897-44b7-a4ab-84500c6f412f"},{"source":"# Count the number of missing values in each column\nprint(planes.isna().sum())\n\n# Find the five percent threshold\nthreshold = len(planes) * 0.05\n\n# Create a filter\ncols_to_drop = planes.columns[planes.isna().sum() <= threshold]\n\n# Drop missing values for columns below the threshold\nplanes.dropna(subset=cols_to_drop, inplace=True)\n\nprint(planes.isna().sum())","metadata":{},"cell_type":"code","id":"5cb5ad45-5ebe-40d0-b763-438893bd0686","outputs":[],"execution_count":null},{"source":"# Check the values of the Additional_Info column\nprint(planes[\"Additional_Info\"].value_counts())\n\n# Create a box plot of Price by Airline\nsns.boxplot(data=planes, x=\"Airline\", y=\"Price\")\n\nplt.show()\n\nHow should you deal with the missing values in \"Additional_Info\" and \"Price\"?\nRemove the \"Additional_Info\" column and impute the median by \"Airline\" for missing values of \"Price\".","metadata":{},"cell_type":"code","id":"f4d057b0-75fb-4c21-a58a-b6d91dbcc4b6","outputs":[],"execution_count":null},{"source":"# Calculate median plane ticket prices by Airline\nairline_prices = planes.groupby(\"Airline\")[\"Price\"].median()\n\nprint(airline_prices)\n\n# Convert to a dictionary\nprices_dict = airline_prices.to_dict()\n\n# Map the dictionary to the missing values\nplanes[\"Price\"] = planes[\"Price\"].fillna(planes[\"Airline\"].map(prices_dict))\n\n# Check for missing values\nprint(planes.isna().sum())","metadata":{},"cell_type":"code","id":"664814f8-7120-46a8-b8f1-cad140e292d3","outputs":[],"execution_count":null},{"source":"# Filter the DataFrame for object columns\nnon_numeric = planes.select_dtypes(\"object\")\n\n# Loop through columns\nfor col in non_numeric.columns:\n  \n  # Print the number of unique values\n  print(f\"Number of unique values in {col} column: \", non_numeric[col].nunique())","metadata":{},"cell_type":"code","id":"489e47a0-a492-4326-863c-d45e2010c6a8","outputs":[],"execution_count":null},{"source":"# Create a list of categories\nflight_categories = [\"Short-haul\", \"Medium\", \"Long-haul\"]\n\n# Create short_flights\nshort_flights = \"^0h|^1h|^2h|^3h|^4h\"\n\n# Create medium_flights\nmedium_flights = \"^5h|^6h|^7h|^8h|^9h\"\n\n# Create long_flights\nlong_flights = \"10h|11h|12h|13h|14h|15h|16h\"","metadata":{},"cell_type":"code","id":"b51ad8d1-34e5-4157-93ea-2f2f3af9badb","outputs":[],"execution_count":null},{"source":"# Create conditions for values in flight_categories to be created\nconditions = [\n    (planes[\"Duration\"].str.contains(short_flights)),\n    (planes[\"Duration\"].str.contains(medium_flights)),\n    (planes[\"Duration\"].str.contains(long_flights))\n]\n\n# Apply the conditions list to the flight_categories\nplanes[\"Duration_Category\"] = np.select(conditions, \n                                        flight_categories,\n                                        default=\"Extreme duration\")\n\n# Plot the counts of each category\nsns.countplot(data=planes, x=\"Duration_Category\")\nplt.show()","metadata":{},"cell_type":"code","id":"aab369cb-e476-4992-b886-c4317a7d5986","outputs":[],"execution_count":null},{"source":"# Preview the column\nprint(planes[\"Duration\"].head())\n\n# Remove the string character\nplanes[\"Duration\"] = planes[\"Duration\"].str.replace(\"h\", \"\")\n\n# Convert to float data type\nplanes[\"Duration\"] = planes[\"Duration\"].astype(float)\n\n# Plot a histogram\nsns.histplot(data=planes, x=\"Duration\", binwidth=10)\nplt.show()","metadata":{},"cell_type":"code","id":"3b3bb10b-5eee-4916-abf0-a03ed20d2b4f","outputs":[],"execution_count":null},{"source":"# Price standard deviation by Airline\nplanes[\"airline_price_st_dev\"] = planes.groupby(\"Airline\")[\"Price\"].transform(lambda x: x.std())\n\nprint(planes[[\"Airline\", \"airline_price_st_dev\"]].value_counts())\n\n# Median Duration by Airline\nplanes[\"airline_median_duration\"] = planes.groupby(\"Airline\")[\"Duration\"].transform(lambda x: x.median())\n\nprint(planes[[\"Airline\",\"airline_median_duration\"]].value_counts())\n\n# Mean Price by Destination\nplanes[\"price_destination_mean\"] = planes.groupby(\"Destination\")[\"Price\"].transform(lambda x: x.mean())\n\nprint(planes[[\"Destination\",\"price_destination_mean\"]].value_counts())","metadata":{},"cell_type":"code","id":"c6768e3b-2dd3-4bc3-ad3a-6b08172148ad","outputs":[],"execution_count":null},{"source":"# Plot a histogram of flight prices\nsns.histplot(data=planes, x=\"Price\")\nplt.show()\n\n# Display descriptive statistics for flight duration\nprint(planes[\"Duration\"].describe())","metadata":{},"cell_type":"code","id":"8b49b708-2efe-44b8-b7df-72f52cd707f4","outputs":[],"execution_count":null},{"source":"# Find the 75th and 25th percentiles\nprice_seventy_fifth = planes[\"Price\"].quantile(0.75)\nprice_twenty_fifth = planes[\"Price\"].quantile(0.25)\n\n# Calculate iqr\nprices_iqr = price_seventy_fifth - price_twenty_fifth\n\n# Calculate the thresholds\nupper = price_seventy_fifth + (1.5 * prices_iqr)\nlower = price_twenty_fifth - (1.5 * prices_iqr)\n\n# Subset the data\nplanes = planes[(planes[\"Price\"] > lower) & (planes[\"Price\"] < upper)]\n\nprint(planes[\"Price\"].describe())","metadata":{},"cell_type":"code","id":"485e3f1b-18f7-4796-a6f3-f2702f0b3069","outputs":[],"execution_count":null},{"source":"# Import divorce.csv, parsing the appropriate columns as dates in the import\ndivorce = pd.read_csv(\"divorce.csv\", parse_dates=[\"divorce_date\", \"dob_man\",\"dob_woman\", \"marriage_date\"])\nprint(divorce.dtypes)\n\n# Convert the marriage_date column to DateTime values\ndivorce[\"marriage_date\"] = pd.to_datetime(divorce[\"marriage_date\"])\n\n# Define the marriage_year column\ndivorce[\"marriage_year\"] = divorce[\"marriage_date\"].dt.year\n\n# Create a line plot showing the average number of kids by year\nsns.lineplot(data=divorce, x=\"marriage_year\", y=\"num_kids\")\nplt.show()\n\n# Create the scatterplot\nsns.scatterplot(data=divorce, x=\"marriage_duration\", y=\"num_kids\")\nplt.show()","metadata":{},"cell_type":"code","id":"af6b42a7-7ca8-4a4b-b1c1-6438dcf57065","outputs":[],"execution_count":null},{"source":"#Seaborn's .pairplot() is excellent for understanding the relationships between several or all variables in a dataset by aggregating pairwise scatter plots in one visual.\n# Create a pairplot for income_woman and marriage_duration\nsns.pairplot(data=divorce, vars=[\"income_woman\", \"marriage_duration\"])\nplt.show()\n\n# Create the scatter plot\nsns.scatterplot(data=divorce, x=\"woman_age_marriage\", y=\"income_woman\", hue=\"education_woman\")\nplt.show()\n\n# Create the KDE plot\nsns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"num_kids\")\nplt.show()\n\n# Update the KDE plot so that marriage duration can't be smoothed too far\nsns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"num_kids\", cut=0)\nplt.show()\n\n# Update the KDE plot to show a cumulative distribution function\nsns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"num_kids\", cut=0, cumulative=True)\nplt.show()","metadata":{},"cell_type":"code","id":"977eed11-9c18-4b78-9ed9-1471fe2c42ec","outputs":[],"execution_count":null},{"source":"## Chapter 4","metadata":{},"cell_type":"markdown","id":"a6f8dd6e-0159-4169-b7b1-52bba9eab09c"},{"source":"# Print the relative frequency of Job_Category\nprint(salaries[\"Job_Category\"].value_counts(normalize=True))\n\n# Cross-tabulate Company_Size and Experience\nprint(pd.crosstab(salaries[\"Company_Size\"], salaries[\"Experience\"]))\n\n# Cross-tabulate Job_Category and Company_Size\nprint(pd.crosstab(salaries[\"Job_Category\"], salaries[\"Company_Size\"],\n            values=salaries[\"Salary_USD\"], aggfunc=\"mean\"))","metadata":{},"cell_type":"code","id":"b47d585e-fc88-4f54-907f-f6ff481a2663","outputs":[],"execution_count":null},{"source":"# Get the month of the response\nsalaries[\"month\"] = salaries[\"date_of_response\"].dt.month\n\n# Extract the weekday of the response\nsalaries[\"weekday\"] = salaries[\"date_of_response\"].dt.weekday\n# Create a heatmap\nsns.heatmap(salaries.corr(), annot=True)\nplt.show()\n\n# Find the 25th percentile\ntwenty_fifth = salaries[\"Salary_USD\"].quantile(0.25)\n\n# Save the median\nsalaries_median = salaries[\"Salary_USD\"].quantile(0.50)\n\n# Gather the 75th percentile\nseventy_fifth = salaries[\"Salary_USD\"].quantile(0.75)\nprint(twenty_fifth, salaries_median, seventy_fifth)\n\n# Create salary labels\nsalary_labels = [\"entry\", \"mid\", \"senior\", \"exec\"]\n\n# Create the salary ranges list\nsalary_ranges = [0, twenty_fifth, salaries_median, seventy_fifth, salaries[\"Salary_USD\"].max()]\n\n# Create salary_level\nsalaries[\"salary_level\"] = pd.cut(salaries[\"Salary_USD\"],\n                                  bins=salary_ranges,\n                                  labels=salary_labels)\n\n# Plot the count of salary levels at companies of different sizes\nsns.countplot(data=salaries, x=\"Company_Size\", hue=\"salary_level\")\nplt.show()","metadata":{},"cell_type":"code","id":"3fcd8ce1-0e9b-4408-b0f1-d64a0beb78e2","outputs":[],"execution_count":null},{"source":"# Filter for employees in the US or GB\nusa_and_gb = salaries[salaries[\"Employee_Location\"].isin([\"US\", \"GB\"])]\n\n# Create a barplot of salaries by location\nsns.barplot(data=usa_and_gb, x=\"Employee_Location\", y=\"Salary_USD\")\nplt.show()\n\n# Create a bar plot of salary versus company size, factoring in employment status\nsns.barplot(data=salaries, x=\"Company_Size\", y=\"Salary_USD\", hue=\"Employment_Status\")\nplt.show()","metadata":{},"cell_type":"code","id":"858e5917-21f1-4f5d-ae13-f44cb51f6db3","outputs":[],"execution_count":null},{"source":"## Recap","metadata":{},"cell_type":"markdown","id":"b2c415f8-efdd-4ed0-9ba3-3dfc790a9261"},{"source":"Your recent learnings\nWhen you left 1 week ago, you worked on Getting to Know a Dataset, the first chapter of the course Exploratory Data Analysis in Python. Here is what you covered in your last lesson:\n\nYou learned about the importance of getting to know a dataset through validation and summarization techniques, focusing on both categorical and numerical data. Key points included:\n\nGrouping Data: Using .groupby() to categorize data, which allows for subsequent aggregation functions like .mean() or .count() to summarize data within each category. For instance, grouping books by genre to find the average rating per genre.\nAggregation Functions: Exploring different aggregating functions such as .sum(), .min(), .max(), .var(), and .std() to describe data characteristics. These functions help in understanding the distribution and variability within the data.\nCombining .groupby() and .agg(): You saw how to apply multiple aggregation functions to grouped data using .agg(), which can take a list of functions or a dictionary specifying functions per column. This is useful for detailed data exploration.\nNamed Aggregations: Creating more readable code and output by naming the results of aggregations, making it easier to understand the applied statistics at a glance.\nFor example, to summarize unemployment rates by continent with mean and standard deviation for 2021, you used:\n\ncontinent_summary = unemployment.groupby(\"continent\").agg( mean_rate_2021=(\"2021\", \"mean\"), std_rate_2021=(\"2021\", \"std\") )\nVisualizing Data with Seaborn: Learning to use Seaborn for creating bar plots that automatically calculate and display the mean of a quantitative variable across categories, including a 95% confidence interval. This visual representation aids in quickly identifying patterns or outliers in the data.\nBy applying these techniques, you've gained valuable skills in data summarization and visualization, essential for any data analysis project.","metadata":{},"cell_type":"code","id":"cba62114-0fc3-4fe2-8139-e4305c0b8c62","outputs":[],"execution_count":null},{"source":"Your recent learnings\nWhen you left 9 days ago, you worked on Data Cleaning and Imputation, chapter 2 of the course Exploratory Data Analysis in Python. Here is what you covered in your last lesson:\n\nYou learned about converting and analyzing categorical data within a DataFrame, focusing on handling job titles in a dataset. Key points covered include:\n\nFiltering Non-Numeric Data: You discovered how to use select_dtypes to isolate non-numeric columns, such as job titles, from a DataFrame. This method helps in focusing on categorical data for analysis.\nAnalyzing Value Frequencies: The nunique method was used to count unique job titles, revealing there were 50 different titles, with \"Research Scientist\" being a less common role.\nString Manipulation for Data Filtering: You learned to use the str.contains method to filter rows based on whether they contain certain keywords, such as \"Scientist\". This is useful for narrowing down data to specific categories of interest.\nCombining Filters: By using the pipe (|) symbol in str.contains, you combined filters to find job titles containing either \"Machine Learning\" or \"AI\", showcasing how to search for multiple conditions within a single column.\nCreating New Categorical Columns: You created a new column, Job_Category, by defining a list of job roles and using NumPy's select function to categorize each job title based on predefined conditions. This allows for a more organized analysis of job categories.\nHere's a snippet of code you worked with:\n\nCreate conditions for values in flight_categories to be created conditions = [ (planes[\"Duration\"].str.contains(short_flights)), (planes[\"Duration\"].str.contains(medium_flights)), (planes[\"Duration\"].str.contains(long_flights)) ] # Apply the conditions list to the flight_categories planes[\"Duration_Category\"] = np.select(conditions, flight_categories, default=\"Extreme duration\")\nThis lesson equipped you with techniques to clean and categorize textual data, making it easier to analyze and visualize.\n\nThe goal of the next lesson is to introduce techniques for visualizing numeric data, enabling you to effectively communicate your findings and insights from your analysis.","metadata":{},"cell_type":"markdown","id":"975b062b-b0f0-41c9-8e59-10268b516b39"},{"source":"Your recent learnings\nWhen you left 12 days ago, you worked on Data Cleaning and Imputation, chapter 2 of the course Exploratory Data Analysis in Python. Here is what you covered in your last lesson:\n\nYou learned about handling numeric data in pandas, focusing on cleaning and transforming data for analysis. Key points included:\n\nRemoving commas and changing data types: You saw how to clean numeric data that was stored as text by removing commas from the Salary_In_Rupees column using Series.str.replace() and then converting the column to a float data type for further analysis.\nCurrency conversion: You learned to create a new column, Salary_USD, by converting Salary_In_Rupees to USD using a conversion rate, demonstrating the process of manipulating and adding new data to a DataFrame.\nCalculating summary statistics: The lesson covered how to use pandas' groupby function and transform method to calculate and add summary statistics like mean and standard deviation to your DataFrame based on specific conditions, such as experience level.\nHandling string values in numeric columns: You tackled cleaning a Duration column in a planes DataFrame, which involved converting string values to a numeric data type to enable analysis.\nFor example, to remove commas and convert a column to float, you used:\n\ndf['Salary_In_Rupees'] = df['Salary_In_Rupees'].str.replace(',', '').astype(float)\nAnd to calculate and add a new column for standard deviation of salaries based on experience:\n\ndf['std_dev_salary'] = df.groupby('Experience')['Salary_USD'].transform(lambda x: x.std())\nThis lesson equipped you with practical skills for cleaning and preparing numeric data, setting a strong foundation for exploratory data analysis and data science projects.\n\nThe goal of the next lesson is to learn how to handle outliers in datasets by identifying, analyzing, and deciding the best approach to deal with them to ensure accurate data analysis.","metadata":{},"cell_type":"markdown","id":"f4b3e552-fa07-46c7-9359-5480615839cb"},{"source":"Your recent learnings\nWhen you left 6 days ago, you worked on Relationships in Data, chapter 3 of the course Exploratory Data Analysis in Python. Here is what you covered in your last lesson:\n\nYou learned about handling outliers, which are observations significantly different from other data points. For instance, in a dataset of house prices, a house priced at five million dollars could be an outlier if the median is $400,000, unless factors like location and size justify the price. Key points covered include:\n\nUnderstanding Outliers: Recognizing that outliers can skew data analyses and may not accurately represent the dataset.\nIdentifying Outliers: Using the interquartile range (IQR) to mathematically define outliers. The IQR is the difference between the 75th and 25th percentiles, and outliers are typically any values 1.5 times the IQR above the 75th percentile or below the 25th percentile.\nCalculating IQR and Outlier Thresholds:\nIQR = Series.quantile(0.75) - Series.quantile(0.25)\nupper_limit = 75th_percentile + 1.5 * IQR\nlower_limit = 25th_percentile - 1.5 * IQR\nDecision Making on Outliers: Deciding whether to keep, adjust, or remove outliers based on their relevance and accuracy.\nImpact of Removing Outliers: Observing how outlier removal can lead to a more normally distributed dataset, which is crucial for many statistical tests and machine learning models.\nYou practiced identifying outliers using visualizations and learned techniques to remove them, thereby preparing your dataset for further analysis.\n\nThe goal of the next lesson is to learn how to enhance the quality of datasets by effectively handling missing data, ensuring more reliable and accurate data analysis.","metadata":{},"cell_type":"markdown","id":"41253821-5b77-470f-9fc5-5abd55bc64cc"},{"source":"When you left 2 days ago, you worked on Turning Exploratory Analysis into Action, chapter 4 of the course Exploratory Data Analysis in Python. Here is what you covered in your last lesson:\n\nYou learned about the relationships between various types of variables in datasets, focusing on categorical variables and their interactions with numerical ones. Specifically, you explored:\n\nThe concept of categorical variables, using the education_man variable to understand the distribution of education levels among men in a dataset. Categorical variables, unlike numerical ones, are best summarized and explored through visualizations rather than numerical summaries.\nHow to visualize the relationship between two variables using histograms and Seaborn's Kernel Density Estimate (KDE) plots. For instance, you examined the relationship between marriage duration and male education level, learning that KDE plots provide a clearer view of distribution peaks across different categories compared to histograms.\nThe importance of adjusting KDE plot parameters, such as the cut keyword, to avoid misleading representations in data visualization. You saw how setting cut=0 can limit the curve to realistic data ranges, eliminating impossible values like negative marriage durations.\nIntegrating categorical data into scatter plots to analyze relationships between numerical variables and categories. You created a scatter plot to investigate the correlation between the age at marriage and education level, using the hue argument to differentiate data points by education level.\nThe practical application of these concepts through exercises, including creating a scatter plot to explore the relationship between women's age at marriage, their income, and education level. The code snippet provided was:\n# Create the scatter plot\nsns.scatterplot(data=divorce, x=\"woman_age_marriage\", y=\"income_woman\", hue=\"education_woman\")\nplt.show()\nLastly, you delved into using KDE plots for comparing distributions across different categories, such as the number of kids in a marriage, to understand how certain factors might influence the duration of a marriage.\nThis lesson equipped you with tools to visualize and analyze the complex relationships between different types of variables in a dataset, enhancing your data exploration and interpretation skills.\n\nThe goal of the next lesson is to teach how to import, convert, manipulate, and visualize DateTime data in pandas for effective time-series analysis.","metadata":{},"cell_type":"markdown","id":"f532b231-d217-4f75-a849-7556e1b6c0d9"},{"source":"","metadata":{},"cell_type":"markdown","id":"fb137533-297d-40c8-9457-da05f1aa4351"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}