{"cells":[{"source":"# Introduction to Statistics in Python\nRun the hidden code cell below to import the data used in this course.","metadata":{},"id":"20c73b4c-626a-4674-b2ca-0f4c47f60628","cell_type":"markdown"},{"source":"Chapter 1","metadata":{},"cell_type":"markdown","id":"32f9096d-45e5-4e21-a74c-6f9403e2c7c0"},{"source":"# Import numpy with alias np\nimport numpy as np\n\n# Filter for Belgium\nbe_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n\n# Filter for USA\nusa_consumption =food_consumption[food_consumption['country'] == 'USA']\n\n# Calculate mean and median consumption in Belgium\nprint(np.mean(be_consumption['consumption']))\nprint(np.median(be_consumption['consumption']))\n\n# Calculate mean and median consumption in USA\nprint(np.mean(usa_consumption['consumption']))\nprint(np.median(usa_consumption['consumption']))","metadata":{},"cell_type":"code","id":"21dbef85-b43e-4c5d-a4c9-4ac57fd147c6","execution_count":null,"outputs":[]},{"source":"# Import numpy as np\nimport numpy as np\n\n# Subset for Belgium and USA only\nbe_and_usa = food_consumption[(food_consumption['country'] == \"Belgium\") | (food_consumption['country'] == 'USA')]\n\n# Group by country, select consumption column, and compute mean and median\nprint(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))","metadata":{"executionTime":16,"lastSuccessfullyExecutedCode":"# Add your code snippets here","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"fb5e8360-ad35-430d-a8d1-559df0c5baae","execution_count":3,"outputs":[]},{"source":"# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Subset for food_category equals rice\nrice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n\n# Histogram of co2_emission for rice and show plot\nplt.hist(rice_consumption['co2_emission']) \nplt.show() # sesgada a la derecha, porque la cola está a la derecha y la mayoría de las observaciones a la izquierda\n\n# Subset for food_category equals rice\nrice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n\n# Calculate mean and median of co2_emission with .agg()\nprint(rice_consumption['co2_emission'].agg([np.mean, np.median]))\n\n","metadata":{},"cell_type":"code","id":"f09fa301-114a-42fd-9369-f236f6d6094d","execution_count":null,"outputs":[]},{"source":"# Calculate the quartiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'], [0, 0.25, 0.5, 0.75, 1]))\n\n# Calculate the quintiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'], [0, 0.2, 0.4, 0.6, 0.8, 1]))\n\n# Calculate the deciles of co2_emission\n\nprint(np.quantile(food_consumption['co2_emission'], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]))","metadata":{},"cell_type":"code","id":"dc77cd82-eb8e-4f5d-bdb7-f7839d20dc2e","execution_count":null,"outputs":[]},{"source":"# Print variance and sd of co2_emission for each food_category\nprint(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Create histogram of co2_emission for food_category 'beef'\nfood_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n# Show plot\nplt.show()\n\n# Create histogram of co2_emission for food_category 'eggs'\nfood_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n# Show plot\nplt.show()","metadata":{},"cell_type":"code","id":"b29e2bb4-bbec-46df-b444-bee95428c777","execution_count":null,"outputs":[]},{"source":"# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\nprint(emissions_by_country)\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\n# Compute the first and third quartiles and IQR of emissions_by_country\n\nq1 = np.quantile(emissions_by_country, 0.25)\nq3 = np.quantile(emissions_by_country, 0.75)\niqr = np.quantile(emissions_by_country, 0.75) - np.quantile(emissions_by_country, 0.25)\n\n# Calculate the lower and upper cutoffs for outliers\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\n\n# Subset emissions_by_country to find outliers\noutliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\nprint(outliers)","metadata":{},"cell_type":"code","id":"ace6d9d9-2ede-4997-b54c-82ef96bf97c8","execution_count":null,"outputs":[]},{"source":"Chapter 2","metadata":{},"cell_type":"markdown","id":"b38d1b1e-b330-408c-8f8e-582aaa2da0ad"},{"source":"# Count the deals for each product\ncounts = amir_deals['product'].value_counts()\nprint(counts)\n\n# Calculate probability of picking a deal with each product\nprobs = counts/178\nprint(probs)\n\n# El 178 salió de hacer un conteo a la columna de productos para ver el total de ventas: amir_deals['product'].count()\n","metadata":{},"cell_type":"code","id":"9bae3f5f-6f8f-4874-9d65-7e0d66c7fe9d","execution_count":null,"outputs":[]},{"source":"# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals without replacement\nsample_without_replacement = amir_deals.sample(5)\nprint(sample_without_replacement)\n\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals with replacement\nsample_with_replacement = amir_deals.sample(5, replace = True)\nprint(sample_with_replacement)","metadata":{},"cell_type":"code","id":"05a51390-72c0-497a-8e52-849f2217e2fb","execution_count":null,"outputs":[]},{"source":"# Create a histogram of restaurant_groups and show plot\nrestaurant_groups['group_size'].hist(bins = [2, 3, 4, 5, 6])\nplt.show()\n\n# Create probability distribution\nsize_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = ['group_size', 'prob']\n\nprint(size_dist)\n\n# Expected value\nexpected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n\n# Subset groups of size 4 or more\ngroups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n\n# Sum the probabilities of groups_4_or_more\nprob_4_or_more = np.sum(groups_4_or_more['prob'])\nprint(prob_4_or_more)","metadata":{},"cell_type":"code","id":"beb5bafa-206d-4663-9cd9-cf95cb496d5e","execution_count":null,"outputs":[]},{"source":"# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Import uniform from scipy.stats\nfrom scipy.stats import uniform\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 = uniform.cdf(5, min_time, max_time)\nprint(prob_less_than_5)\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\nprint(prob_greater_than_5)\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - uniform.cdf(10, min_time, max_time)\nprint(prob_between_10_and_20)","metadata":{},"cell_type":"code","id":"105cd330-ec85-4112-970d-b6ab04eb9c7d","execution_count":null,"outputs":[]},{"source":"# Set random seed to 334\nnp.random.seed(334)\n\n# Import uniform\nfrom scipy.stats import uniform\n\n# Generate 1000 wait times between 0 and 30 mins\nwait_times = uniform.rvs(0, 30, size=1000)\n\n# Create a histogram of simulated times and show plot\nplt.hist(wait_times)\nplt.show()","metadata":{},"cell_type":"code","id":"060c5536-1dde-415b-b60c-f254a71e33f6","execution_count":null,"outputs":[]},{"source":"# Import binom from scipy.stats\nfrom scipy.stats import binom\n\n# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate a single deal\nprint(binom.rvs(1, 0.3, size=1)) \n\n# Simulate 1 week of 3 deals\nprint(binom.rvs(3, 0.3, size=1))\n\n# Simulate 52 weeks of 3 deals\ndeals = binom.rvs(3, 0.3, size=52)\n\n# Print mean deals won per week\nprint(np.mean(deals))","metadata":{},"cell_type":"code","id":"ccba5d65-adf3-47ce-a3c3-ba1fb21039ab","execution_count":null,"outputs":[]},{"source":"# Probability of closing 3 out of 3 deals\nprob_3 = binom.pmf(3, 3, 0.3)\n\nprint(prob_3)\n\n# Probability of closing <= 1 deal out of 3 deals\nprob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)\n\nprint(prob_less_than_or_equal_1)\n\n# Probability of closing > 1 deal out of 3 deals\nprob_greater_than_1 = 1 - binom.cdf(1, 3, 0.3)\n\nprint(prob_greater_than_1)","metadata":{},"cell_type":"code","id":"9975286a-aeb9-4f2c-87ad-f591d227466f","execution_count":null,"outputs":[]},{"source":"# Expected number won with 30% win rate\nwon_30pct = 3 * 0.3\nprint(won_30pct)\n\n# Expected number won with 25% win rate\nwon_25pct = 3 * 0.25\nprint(won_25pct)\n\n# Expected number won with 35% win rate\nwon_35pct = 3 * 0.35\nprint(won_35pct)","metadata":{},"cell_type":"code","id":"ec40ae63-69bb-4274-be1e-a7b06d78a812","execution_count":null,"outputs":[]},{"source":"# Histogram of amount with 10 bins and show plot\namir_deals['amount'].hist(bins=10)\nplt.show()","metadata":{},"cell_type":"code","id":"ca2af1b1-5d45-427c-94f4-fada42ce6264","execution_count":null,"outputs":[]},{"source":"# Probability of deal < 7500\nprob_less_7500 = norm.cdf(7500, 5000, 2000)\n\nprint(prob_less_7500)\n\n# Probability of deal > 1000\nprob_over_1000 = 1 - norm.cdf(1000, 5000, 2000)\n\nprint(prob_over_1000)\n\n# Probability of deal between 3000 and 7000\nprob_3000_to_7000 = norm.cdf(7000, 5000, 2000) - norm.cdf(3000, 5000, 2000)\n\nprint(prob_3000_to_7000)\n\n# Calculate amount that 25% of deals will be less than\npct_25 = norm.ppf(0.25, 5000, 2000)\n\nprint(pct_25)","metadata":{},"cell_type":"code","id":"be41f125-8cad-4a75-a04c-148da370a58a","execution_count":null,"outputs":[]},{"source":"# Calculate new average amount\nnew_mean = 5000 * 1.2\n\n# Calculate new standard deviation\nnew_sd = 2000 * 1.3\n\n# Simulate 36 new sales\nnew_sales = norm.rvs(new_mean, new_sd, size=36)\n\n#-Create histogram and show\nplt.hist(new_sales)\nplt.show()","metadata":{},"cell_type":"code","id":"e3a7ff89-18c8-463e-aab4-0d056b59f122","execution_count":null,"outputs":[]},{"source":"Which market is better?\nThe key metric that the company uses to evaluate salespeople is the percent of sales they make over $1000 since the time put into each sale is usually worth a bit more than that, so the higher this metric, the better the salesperson is performing.\n\nRecall that Amir's current sales amounts have a mean of $5000 and a standard deviation of $2000, and Amir's predicted amounts in next quarter's market have a mean of $6000 and a standard deviation of $2600.\n\nnorm from scipy.stats is imported.\n\nBased only on the metric of percent of sales over $1000, does Amir perform better in the current market or the predicted market?\n\nIn [3]:\n1-norm.cdf(1000, 5000, 2000)\nOut[3]:\n0.9772498680518208\nIn [4]:\n1-norm.cdf(1000, 6000, 2600)\nOut[4]:\n0.9727648049862613\n\n# Bastante parecido en ambos casos","metadata":{},"cell_type":"code","id":"7e4be412-9387-418a-ac78-8f4e4bf254b2","execution_count":null,"outputs":[]},{"source":"# Create a histogram of num_users and show\namir_deals['num_users'].hist() \nplt.show()\n\n# Set seed to 104\nnp.random.seed(104)\n\n# Sample 20 num_users with replacement from amir_deals\nsamp_20 = amir_deals['num_users'].sample(20, replace = True)\n\n# Take mean of samp_20\nprint(np.mean(samp_20))\n\n# Set seed to 104\nnp.random.seed(104)\n\n# Sample 20 num_users with replacement from amir_deals and take mean\nsamp_20 = amir_deals['num_users'].sample(20, replace=True)\nnp.mean(samp_20)\n\nsample_means = []\n# Loop 100 times\nfor i in range(100):\n  # Take sample of 20 num_users\n  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n  # Calculate mean of samp_20\n  samp_20_mean = np.mean(samp_20)\n  # Append samp_20_mean to sample_means\n  sample_means.append(samp_20_mean)\n  \nprint(sample_means)\n\n# Convert to Series and plot histogram\nsample_means_series = pd.Series(sample_means)\nsample_means_series.hist()\n# Show plot\nplt.show()","metadata":{},"cell_type":"code","id":"d62e9671-095c-44ef-a7a5-3098ed751340","execution_count":null,"outputs":[]},{"source":"# Set seed to 321\nnp.random.seed(321)\n\nsample_means = []\n# Loop 30 times to take 30 means\nfor i in range(30):\n  # Take sample of size 20 from num_users col of all_deals with replacement\n  cur_sample = all_deals['num_users'].sample(20, replace=True)\n  # Take mean of cur_sample\n  cur_mean = np.mean(cur_sample)\n  # Append cur_mean to sample_means\n  sample_means.append(cur_mean)\n\n# Print mean of sample_means\nprint(np.mean(sample_means))\n\n# Print mean of num_users in amir_deals\nprint(np.mean(amir_deals['num_users']))","metadata":{},"cell_type":"code","id":"9a5ae4ab-6820-416c-944d-0b8c7215a094","execution_count":null,"outputs":[]},{"source":"# Import poisson from scipy.stats and calculate the probability that Amir responds to 5 leads in a day, given that he responds to an average of 4.\n\n# Import poisson from scipy.stats\nfrom scipy.stats import poisson\n\n# Probability of 5 responses\nprob_5 = poisson.pmf(5, 4)\n\nprint(prob_5)\n\n# Amir's coworker responds to an average of 5.5 leads per day. What is the probability that she answers 5 leads in a day?\n\n# Import poisson from scipy.stats\nfrom scipy.stats import poisson\n\n# Probability of 5 responses\nprob_coworker = prob_5 = poisson.pmf(5, 5.5)\n\nprint(prob_coworker)\n\n# What's the probability that Amir responds to 2 or fewer leads in a day?\n\n# Import poisson from scipy.stats\nfrom scipy.stats import poisson\n\n# Probability of 2 or fewer responses\nprob_2_or_less = poisson.cdf(2, 4)\n\nprint(prob_2_or_less)\n\n# What's the probability that Amir responds to more than 10 leads in a day?\n\n# Import poisson from scipy.stats\nfrom scipy.stats import poisson\n\n# Probability of > 10 responses\nprob_over_10 = 1 - poisson.cdf(10, 4)\n\nprint(prob_over_10)","metadata":{},"cell_type":"code","id":"b8d56dba-b02c-43bc-929a-2f5a15eb5997","execution_count":null,"outputs":[]},{"source":"#To further evaluate Amir's performance, you want to know how much time it takes him to respond to a lead after he opens it. On average, he responds to 1 request every 2.5 hours. In this exercise, you'll calculate probabilities of different amounts of time passing between Amir receiving a lead and sending a response.\n\n# Import expon from scipy.stats\nfrom scipy.stats import expon\n\n# Print probability response takes < 1 hour\nprint(expon.cdf(1, scale=2.5))\n\n# Import expon from scipy.stats\nfrom scipy.stats import expon\n\n# Print probability response takes > 4 hours\nprint(1- expon.cdf(4, scale=2.5))\n\n# Import expon from scipy.stats\nfrom scipy.stats import expon\n\n# Print probability response takes 3-4 hours\nprint(expon.cdf(4, scale=2.5) - expon.cdf(3, scale=2.5))","metadata":{},"cell_type":"code","id":"82d78c7e-6304-4457-8286-60a457f21ace","execution_count":null,"outputs":[]},{"source":"Chapter 4","metadata":{},"cell_type":"markdown","id":"1ef2838a-4cc2-4052-be25-d99c810da731"},{"source":"# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(x=\"life_exp\", y=\"happiness_score\", data=world_happiness)\n\n# Show plot\nplt.show()\n\n# Create scatterplot of happiness_score vs life_exp with trendline\nsns.lmplot(x=\"life_exp\", y=\"happiness_score\", data=world_happiness, ci=None)\n\n# Show plot\nplt.show()\n\n# Correlation between life_exp and happiness_score\ncor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n\nprint(cor)","metadata":{},"cell_type":"code","id":"011920f0-ab27-4383-bf31-51cb22845ca6","execution_count":null,"outputs":[]},{"source":"# Scatterplot of gdp_per_cap and life_exp\nsns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)\n\n# Show plot\nplt.show()\n  \n# Correlation between gdp_per_cap and life_exp\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\n\nprint(cor)","metadata":{},"cell_type":"code","id":"93c72752-b76d-4ca0-a21c-6c1e183c630c","execution_count":null,"outputs":[]},{"source":"# Scatterplot of happiness_score vs. gdp_per_cap\nsns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n\n# Create log_gdp_per_cap column\nworld_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n\n# Scatterplot of happiness_score vs. log_gdp_per_cap\nsns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)","metadata":{},"cell_type":"code","id":"18a83fc8-3fce-47fb-81af-89c5cfb5df18","execution_count":null,"outputs":[]},{"source":"# Scatterplot of grams_sugar_per_day and happiness_score\nsns.scatterplot(x='grams_sugar_per_day', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor = world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])\nprint(cor)","metadata":{},"cell_type":"code","id":"e5af33b0-70d0-4ec5-bbb5-cfb81fff7b1d","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"774700f4-3fd4-4863-991f-f4ea0d290851","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"ab4a445f-ff74-4d34-afa5-54fd4f8d3fbd","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"e03d2639-294f-4b3c-a1f2-ce462f45e90e","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"06e5244c-5d1b-4991-96ae-de9f93c8f52f","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"cc0d6bca-7005-4167-a33b-9ccd9d902007","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"fc2135b2-f6d1-4b23-93da-9d17c6bf3c27","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}