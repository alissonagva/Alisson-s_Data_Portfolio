{"cells":[{"source":"# Data Manipulation with pandas\nRun the hidden code cell below to import the data used in this course.","metadata":{"id":"bA5ajAmk7XH6"},"id":"prostate-arizona","cell_type":"markdown"},{"source":"# Import the course packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import the four datasets\navocado = pd.read_csv(\"datasets/avocado.csv\")\nhomelessness = pd.read_csv(\"datasets/homelessness.csv\")\ntemperatures = pd.read_csv(\"datasets/temperatures.csv\")\nwalmart = pd.read_csv(\"datasets/walmart.csv\")\n","metadata":{"scrolled":true,"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":180,"lastSuccessfullyExecutedCode":"# Import the course packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import the four datasets\navocado = pd.read_csv(\"datasets/avocado.csv\")\nhomelessness = pd.read_csv(\"datasets/homelessness.csv\")\ntemperatures = pd.read_csv(\"datasets/temperatures.csv\")\nwalmart = pd.read_csv(\"datasets/walmart.csv\")\n"},"id":"2e25fdd8-4d84-45bc-80f0-949917e00a17","cell_type":"code","execution_count":3,"outputs":[]},{"source":"","metadata":{},"cell_type":"markdown","id":"1836df0c-dfd3-4f7b-a130-3e916dcb6c18"},{"source":"## Take Notes\n\nAdd notes about the concepts you've learned and code cells with code you want to keep.","metadata":{},"id":"6fad679d","cell_type":"markdown"},{"source":"_Add your notes here_","metadata":{},"id":"e9a448e0","cell_type":"markdown"},{"source":"# Import NumPy and create custom IQR function\nimport numpy as np\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n\n# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median]))","metadata":{"executionTime":612,"lastSuccessfullyExecutedCode":"# Import NumPy and create custom IQR function\nimport numpy as np\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n\n# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median]))"},"id":"893055c9","cell_type":"code","execution_count":null,"outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.75\u001b[39m) \u001b[38;5;241m-\u001b[39m column\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.25\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msales\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature_c\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuel_price_usd_per_l\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munemployment\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39magg([iqr,np\u001b[38;5;241m.\u001b[39mmedian]))\n","\u001b[0;31mNameError\u001b[0m: name 'sales' is not defined"],"ename":"NameError","evalue":"name 'sales' is not defined"}]},{"source":"store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\nprint(store_types.head())\n\n# Drop duplicate store/department combinations\nstore_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\nprint(store_depts.head())\n\n# Subset the rows where is_holiday is True and drop duplicate dates\nholiday_dates = sales[sales['is_holiday']].drop_duplicates('date')\n\n# Print date col of holiday_dates\nprint(holiday_dates['date'])","metadata":{},"cell_type":"code","id":"4b82eeed-6d70-4aee-b614-5beba99d2b33","execution_count":null,"outputs":[]},{"source":"# Count the number of stores of each type\nstore_counts = store_types['type'].value_counts()\nprint(store_counts)\n\n# Get the proportion of stores of each type\nstore_props = store_types[\"type\"].value_counts(normalize=True)\nprint(store_props)\n\n# Count the number of each department number and sort\ndept_counts_sorted = store_depts[\"department\"].value_counts(sort=True)\nprint(dept_counts_sorted)\n\n# Get the proportion of departments of each number and sort\ndept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\nprint(dept_props_sorted)","metadata":{},"cell_type":"code","id":"9d66b427-fc0f-44df-9c54-af1c1e099d48","execution_count":null,"outputs":[]},{"source":"# Calc total weekly sales\nsales_all = sales[\"weekly_sales\"].sum()\n\n# Subset for type A stores, calc total weekly sales\nsales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n\n# Subset for type B stores, calc total weekly sales\nsales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n\n# Subset for type C stores, calc total weekly sales\nsales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n\n# Get proportion for each type\nsales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\nprint(sales_propn_by_type)","metadata":{},"cell_type":"code","id":"71092d2d-8efd-43e2-93a8-0221c53ead25","execution_count":null,"outputs":[]},{"source":"# From previous step\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Group by type and is_holiday; calc total weekly sales\nsales_by_type_is_holiday = sales.groupby([\"type\", 'is_holiday'])[\"weekly_sales\"].sum()\nprint(sales_by_type_is_holiday)","metadata":{},"cell_type":"code","id":"d3077ad5-d0a3-4846-8241-717d70326038","execution_count":null,"outputs":[]},{"source":"# Import numpy with the alias np\nimport numpy as np\n\n# For each store type, aggregate weekly_sales: get min, max, mean, and median\nsales_stats = sales.groupby('type')['weekly_sales'].agg([min, max, np.mean, np.median])\n\n# Print sales_stats\nprint(sales_stats)\n\n# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\nunemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([min, max, np.mean, np.median])\n\n\n# Print unemp_fuel_stats\nprint(unemp_fuel_stats)","metadata":{},"cell_type":"code","id":"2841ef32-ff7f-45c3-950d-9f026ad3640f","execution_count":null,"outputs":[]},{"source":"# Pivot for mean weekly_sales for each store type\nmean_sales_by_type = sales.pivot_table(values = 'weekly_sales', index = 'type')\n\n# Print mean_sales_by_type\nprint(mean_sales_by_type)\n\n# Import NumPy as np\nimport numpy as np\n\n# Pivot for mean and median weekly_sales for each store type\nmean_med_sales_by_type = sales.pivot_table(values = 'weekly_sales', index = 'type', aggfunc = [np.mean, np.median])\n\n# Print mean_med_sales_by_type\nprint(mean_med_sales_by_type)\n\n# Pivot for mean weekly_sales by store type and holiday \nmean_sales_by_type_holiday = sales.pivot_table(values = 'weekly_sales', index = 'type', columns = 'is_holiday')\n\n# Print mean_sales_by_type_holiday\nprint(mean_sales_by_type_holiday)\n\n# Print mean weekly_sales by department and type; fill missing values with 0\nprint(sales.pivot_table(values = 'weekly_sales', index = 'department', columns = 'type', fill_value = 0))\n\n# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\nprint(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value = 0, margins = True))","metadata":{},"cell_type":"code","id":"bd1fcc44-47b7-40a6-b34b-03018d384d16","execution_count":null,"outputs":[]},{"source":"# Chapter 3: Indexes and .loc\n\n\n# Look at temperatures\nprint(temperatures)\n\n# Set the index of temperatures to city\ntemperatures_ind = temperatures.set_index('city')\n\n# Look at temperatures_ind\nprint(temperatures_ind)\n\n# Reset the temperatures_ind index, keeping its contents\nprint(temperatures_ind.reset_index())\n\n# Reset the temperatures_ind index, dropping its contents\nprint(temperatures_ind.reset_index(drop=True))","metadata":{},"cell_type":"code","id":"bf552fab-0523-44a9-86a1-74cf8d3dbb12","execution_count":null,"outputs":[]},{"source":"# Make a list of cities to subset on\ncities = [\"Moscow\", \"Saint Petersburg\"]\n\n# Subset temperatures using square brackets\nprint(temperatures[temperatures['city'].isin(cities)])\n\n# Subset temperatures_ind using .loc[]\nprint(temperatures_ind.loc[[\"Moscow\", \"Saint Petersburg\"]])","metadata":{},"cell_type":"code","id":"de77e962-652c-46de-9fd2-83646ddadca6","execution_count":null,"outputs":[]},{"source":"# Index temperatures by country & city\ntemperatures_ind = temperatures.set_index([\"country\", \"city\"])\n\n# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\nrows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n\n# Subset for rows to keep\nprint(temperatures_ind.loc[rows_to_keep])","metadata":{},"cell_type":"code","id":"dc691a01-fd7c-45d8-80bb-6fd31b8f38a2","execution_count":null,"outputs":[]},{"source":"# Sort temperatures_ind by index values\nprint(temperatures_ind.sort_index())\n\n# Sort temperatures_ind by index values at the city level\nprint(temperatures_ind.sort_index(level=\"city\"))\n\n# Sort temperatures_ind by country then descending city\nprint(temperatures_ind.sort_index(level=[\"country\", \"city\"], ascending = [True, False]))","metadata":{},"cell_type":"code","id":"71785490-ab09-4524-ac73-0eb52cc99e74","execution_count":null,"outputs":[]},{"source":"# Sort the index of temperatures_ind\ntemperatures_srt = temperatures_ind.sort_index()\n\n# Subset rows from Pakistan to Russia\nprint(temperatures_srt.loc['Pakistan':'Russia'])\n\n# Try to subset rows from Lahore to Moscow\nprint(temperatures_srt.loc['Lahore':'Moscow'])\n\n# Subset rows from Pakistan, Lahore to Russia, Moscow\nprint(temperatures_srt.loc[('Pakistan','Lahore'):('Russia','Moscow') ])","metadata":{},"cell_type":"code","id":"7d55fcbc-2e6b-4fa1-996c-b3098ab34a2f","execution_count":null,"outputs":[]},{"source":"# Subset rows from India, Hyderabad to Iraq, Baghdad\nprint(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad')])\n\n# Subset columns from date to avg_temp_c\nprint(temperatures_srt.loc[:, 'date':'avg_temp_c'])\n\n# Subset in both directions at once\nprint(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad'), 'date':'avg_temp_c'])","metadata":{},"cell_type":"code","id":"06bdeb77-bc05-48a7-870b-eef4daa15185","execution_count":null,"outputs":[]},{"source":"# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\ntemperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-31\")]\n\nprint(temperatures_bool)\n\n# Set date as the index and sort the index\ntemperatures_ind = temperatures.set_index(\"date\").sort_index()\n\n# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\nprint(temperatures_ind.loc[\"2010\":\"2011\"])\n\n# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\nprint(temperatures_ind.loc[\"2010-08\":\"2011-02\"])","metadata":{},"cell_type":"code","id":"4d900eda-c46d-46c2-aa56-e21d6b4cc66a","execution_count":null,"outputs":[]},{"source":"# Get 23rd row, 2nd column (index 22, 1)\nprint(temperatures.iloc[22,1])\n\n# Use slicing to get the first 5 rows\nprint(temperatures.iloc[:5])\n\n# Use slicing to get columns 3 to 4\nprint(temperatures.iloc[:,2:4])\n\n# Use slicing in both directions at once\nprint(temperatures.iloc[:5,2:4])","metadata":{},"cell_type":"code","id":"99e6ad0a-b928-4d8d-ae00-7905211b5812","execution_count":null,"outputs":[]},{"source":"# Add a year column to temperatures\ntemperatures[\"year\"] = temperatures[\"date\"].dt.year\n\n# Pivot avg_temp_c by country and city vs year\ntemp_by_country_city_vs_year = temperatures.pivot_table('avg_temp_c', index=['country', 'city'], columns=\"year\")\n\n# See the result\nprint(temp_by_country_city_vs_year)","metadata":{},"cell_type":"code","id":"b6564251-b70b-477e-88af-19ae69ae221b","execution_count":null,"outputs":[]},{"source":"# Subset for Egypt to India\ntemp_by_country_city_vs_year.loc['Egypt':'India']\n\n# Subset for Egypt, Cairo to India, Delhi\ntemp_by_country_city_vs_year.loc[('Egypt', 'Cairo'):('India', 'Delhi')]\n\n# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\ntemp_by_country_city_vs_year.loc[('Egypt', 'Cairo'):('India', 'Delhi'), 2005:2010]","metadata":{},"cell_type":"code","id":"b8cce17b-80c5-4a29-8deb-7c98ec9c6c43","execution_count":null,"outputs":[]},{"source":"# Get the worldwide mean temp by year\nmean_temp_by_year = temp_by_country_city_vs_year.mean()\n\n# Filter for the year that had the highest mean temp\nprint(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n\n# Get the mean temp by city\nmean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\"columns\")\n\n# Filter for the city that had the lowest mean temp\nprint(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])","metadata":{},"cell_type":"code","id":"fd883004-104e-4f23-96c4-4336d9e0a914","execution_count":null,"outputs":[]},{"source":"Chapter 4","metadata":{},"cell_type":"markdown","id":"6f98c96f-9306-43fc-a6c5-19cc03ecf0ba"},{"source":"# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Look at the first few rows of data\nprint(avocados.head())\n\n# Get the total number of avocados sold of each size\nnb_sold_by_size = avocados.groupby('size')['nb_sold'].sum()\n\n# Create a bar plot of the number of avocados sold by size\nnb_sold_by_size.plot(kind = 'bar')\n\n# Show the plot\nplt.show()","metadata":{},"cell_type":"code","id":"f35d7a2d-1e4c-429e-b795-918e568779a2","execution_count":null,"outputs":[]},{"source":"# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Get the total number of avocados sold on each date\nnb_sold_by_date = avocados.groupby('date')['nb_sold'].sum()\n\n# Create a line plot of the number of avocados sold by date\nnb_sold_by_date.plot(kind = 'line')\n\n# Show the plot\nplt.show()","metadata":{},"cell_type":"code","id":"4daaa2a9-b7cd-4c64-bf86-3f50bd403c07","execution_count":null,"outputs":[]},{"source":"# Scatter plot of avg_price vs. nb_sold with title\navocados.plot(x = 'nb_sold', y = 'avg_price', kind = 'scatter', title = \"Number of avocados sold vs. average price\")\n\n# Show the plot\nplt.show()","metadata":{},"cell_type":"code","id":"9d5ebcb9-8e67-46ce-86ff-2bce0e9e4dd8","execution_count":null,"outputs":[]},{"source":"# Histogram of conventional avg_price \navocados[avocados['type'] == 'conventional']['avg_price'].hist()\n\n# Histogram of organic avg_price\navocados[avocados['type'] == 'organic']['avg_price'].hist()\n\n# Add a legend\nplt.legend(['conventional','organic'])\n\n# Show the plot\nplt.show()","metadata":{},"cell_type":"code","id":"78516162-2e17-4f5d-bdb8-436f84288693","execution_count":null,"outputs":[]},{"source":"# Modify histogram transparency to 0.5 \navocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5)\n\n# Modify histogram transparency to 0.5\navocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5)\n\n# Add a legend\nplt.legend([\"conventional\", \"organic\"])\n\n# Show the plot\nplt.show()","metadata":{},"cell_type":"code","id":"6ba9cf28-2927-4177-b5a2-ac757914edfd","execution_count":null,"outputs":[]},{"source":"# Modify bins to 20\navocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\n\n# Modify bins to 20\navocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\n\n# Add a legend\nplt.legend([\"conventional\", \"organic\"])\n\n# Show the plot\nplt.show()","metadata":{},"cell_type":"code","id":"481743ef-1201-4514-ac3a-8ba2038b5fa5","execution_count":null,"outputs":[]},{"source":"# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Check individual values for missing values\nprint(avocados_2016.isna())\n\n# Check each column for missing values\nprint(avocados_2016.isna().any())\n\n# Bar plot of missing values by variable\navocados_2016.isna().sum().plot(kind='bar')\n\n# Show plot\nplt.show()","metadata":{},"cell_type":"code","id":"c8771807-e0a7-4bb3-a476-5bfe01df56af","execution_count":null,"outputs":[]},{"source":"# Remove rows with missing values\navocados_complete = avocados_2016.dropna()\n\n# Check if any columns contain missing values\nprint(avocados_complete.isna().any())","metadata":{},"cell_type":"code","id":"6fbdd830-76f3-4bde-bfa2-31283dddbe64","execution_count":null,"outputs":[]},{"source":"# List the columns with missing values\ncols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n\n# Create histograms showing the distributions cols_with_missing\navocados_2016[cols_with_missing].hist()\n\n# Show the plot\nplt.show()","metadata":{},"cell_type":"code","id":"58cbb180-837e-4423-b4bb-be80712122cd","execution_count":null,"outputs":[]},{"source":"# From previous step\ncols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\navocados_2016[cols_with_missing].hist()\nplt.show()\n\n# Fill in missing values with 0\navocados_filled = avocados_2016.fillna(0)\n\n# Create histograms of the filled columns\navocados_filled[cols_with_missing].hist()\n\n# Show the plot\nplt.show()","metadata":{},"cell_type":"code","id":"c3add5d0-7d10-4391-9a88-371f7f3dc824","execution_count":null,"outputs":[]},{"source":"Chapter 4","metadata":{},"cell_type":"markdown","id":"0e19fbd6-f0a0-4705-93fc-2101c76721c2"},{"source":"# Create a list of dictionaries with new data\navocados_list = [\n    {'date': '2019-11-03', 'small_sold': 10376832, 'large_sold': 7835071},\n    {'date': '2019-11-10', 'small_sold': 10717154, 'large_sold': 8561348},\n]\n\n# Convert list into DataFrame\navocados_2019 = pd.DataFrame(avocados_list)\n\n# Print the new DataFrame\nprint(avocados_2019)","metadata":{},"cell_type":"code","id":"6caac407-9232-4dd8-95a4-7f85137635c4","execution_count":null,"outputs":[]},{"source":"# Create a dictionary of lists with new data\navocados_dict = {\n  \"date\": [\"2019-11-17\", \"2019-12-01\"\t],\n  \"small_sold\": [10859987, 9291631],\n  \"large_sold\": [7674135, 6238096]\n}\n\n# Convert dictionary into DataFrame\navocados_2019 = pd.DataFrame(avocados_dict)\n\n# Print the new DataFrame\nprint(avocados_2019)","metadata":{},"cell_type":"code","id":"47e30d39-6758-4520-8b25-7c0505db0dea","execution_count":null,"outputs":[]},{"source":"# Read CSV as DataFrame called airline_bumping\nairline_bumping = pd.read_csv(\"airline_bumping.csv\")\n\n# Take a look at the DataFrame\nprint(airline_bumping.head())","metadata":{},"cell_type":"code","id":"012f23b3-bfb6-47de-b857-d6b229d66248","execution_count":null,"outputs":[]},{"source":"# Read CSV as DataFrame called airline_bumping\nairline_bumping = pd.read_csv(\"airline_bumping.csv\")\n\n# Take a look at the DataFrame\nprint(airline_bumping.head())","metadata":{},"cell_type":"code","id":"4f264838-a73d-4281-9865-ef8fcf800a97","execution_count":null,"outputs":[]},{"source":"# From previous step\nairline_bumping = pd.read_csv(\"airline_bumping.csv\")\nprint(airline_bumping.head())\n\n# For each airline, select nb_bumped and total_passengers and sum\nairline_totals = airline_bumping.groupby('airline')[['nb_bumped', 'total_passengers']].sum()","metadata":{},"cell_type":"code","id":"63058f37-e24b-4506-81d3-f610407eafa1","execution_count":null,"outputs":[]},{"source":"# From previous steps\nairline_bumping = pd.read_csv(\"airline_bumping.csv\")\nprint(airline_bumping.head())\nairline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n\n# Create new col, bumps_per_10k: no. of bumps per 10k passengers for each airline\nairline_totals[\"bumps_per_10k\"] = airline_totals['nb_bumped'] / airline_totals['total_passengers'] * 10000","metadata":{},"cell_type":"code","id":"78e8a77e-0637-4456-a98e-72259da9d524","execution_count":null,"outputs":[]},{"source":"# From previous steps\nairline_bumping = pd.read_csv(\"airline_bumping.csv\")\nprint(airline_bumping.head())\nairline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\nairline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n\n# Print airline_totals\nprint(airline_totals)","metadata":{},"cell_type":"code","id":"1e73ef7d-97d9-4e43-a747-3df00100480d","execution_count":null,"outputs":[]},{"source":"# Create airline_totals_sorted \nairline_totals_sorted = airline_totals.sort_values(\"bumps_per_10k\", ascending=False)\n\n# Print airline_totals_sorted\nprint(airline_totals_sorted)\n\n# Save as airline_totals_sorted.csv\nairline_totals_sorted.to_csv('airline_totals_sorted.csv')","metadata":{},"cell_type":"code","id":"891e879f-f493-4197-af2d-6f3eee4e9482","execution_count":null,"outputs":[]},{"source":"## Explore Datasets\nUse the DataFrames imported in the first cell to explore the data and practice your skills!\n- Print the highest weekly sales for each `department` in the `walmart` DataFrame. Limit your results to the top five departments, in descending order. If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/aggregating-dataframes?ex=1).\n- What was the total `nb_sold` of organic avocados in 2017 in the `avocado` DataFrame? If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/slicing-and-indexing-dataframes?ex=6).\n- Create a bar plot of the total number of homeless people by region in the `homelessness` DataFrame. Order the bars in descending order. Bonus: create a horizontal bar chart. If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/creating-and-visualizing-dataframes?ex=1).\n- Create a line plot with two lines representing the temperatures in Toronto and Rome. Make sure to properly label your plot. Bonus: add a legend for the two lines. If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/creating-and-visualizing-dataframes?ex=1).","metadata":{},"id":"c09c5c3a","cell_type":"markdown"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}